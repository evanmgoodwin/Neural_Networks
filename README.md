# Neural_Networks

## Challenge Summary

For the original run-through, I built a model with two layers, eight neurons in the first layer and five in the second. This resulted in an unacceptable accuracy score of 53%, so I rebuilt the model using 100 neurons in the first layer, and 35 in the second, hoping to optimize the performance. The results were the same, so I next tried rebuilding the model with three hidden layers, with 20, 10 and 5 neurons. The accuracy score of this third model was still at 53%. In additon to chaning the number of neurons and hidden layers, I built another model using the tanh output activation function. This brought the accuracy down to 47%. One thing I intentionally did not try to was to increase the amount of epochs, or iterations. It was clear from looking at the model history that adding more would not impact the performance, as the accuracy score had already leveled out by the time the model hit 100 epochs. Another thing I could have tried was to decrease the number of input features, but looking at the DataFrame, all of them seemed important enough to keep to have a positive impact on the model, with not too many outliers. Another type of model I would suggest using would be a Random Forest Classifier. Random Forests are particularly good at handling nonlinear data, dealing with weak correlations, and have a higher performance speed. It would be worthwhile to try running the data through a Random Forest model and comparing the results.
